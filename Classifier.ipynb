{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed50074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "import random\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import noisereduce as nr\n",
    "# Load the official Silero VAD model\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)\n",
    "(get_speech_timestamps, _, _, _, collect_chunks) = utils\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d989a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')\n",
    "class_names =list(pd.read_csv(class_map_path)['display_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b214d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df[\"scientific_name\"] = label_encoder.fit_transform(df[\"scientific_name\"])\n",
    "df['filename'] = 'train_audio/' + df['filename'].str[:]\n",
    "df = df[['filename', 'scientific_name']]\n",
    "df.rename(columns={'scientific_name': 'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d9a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.groupby('class').head(5).reset_index(drop=True)\n",
    "excluded_df = df.groupby('class').apply(lambda x: x.iloc[5:]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = new_df['class'].to_numpy()\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf8f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Util\n",
    "class AudioUtil():\n",
    "  # Load Audio Data from Source. Returns Tensor and Sample Rate of that Audio File.\n",
    "  def read_file(audio_file_path, target_sr=16000):\n",
    "    waveform, sr = torchaudio.load(audio_file_path)\n",
    "    # converting to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(sr, target_sr) # Converting raw data to target_sr\n",
    "        waveform = resampler(waveform)\n",
    "    denoised_waveform = torch.tensor(nr.reduce_noise(y=waveform, sr=sr))\n",
    "    rms = torch.sqrt(torch.mean(denoised_waveform ** 2))\n",
    "    normalized_waveform =  denoised_waveform * (0.1 / rms) if rms > 0 else waveform\n",
    "    return (normalized_waveform, target_sr)\n",
    "  \n",
    "  def extract_non_speech_segments(audio_file, segment_length_sec=1.0, no_of_segments = 3):\n",
    "    waveform, sr = audio_file\n",
    "    waveform = waveform.squeeze(0)\n",
    "\n",
    "    # Use Silero VAD to get timestamps of speech\n",
    "    speech_timestamps = get_speech_timestamps(waveform, model, sampling_rate=sr)\n",
    "\n",
    "    # Convert speech regions into a mask\n",
    "    speech_mask = torch.zeros_like(waveform, dtype=torch.bool)\n",
    "    for ts in speech_timestamps:\n",
    "        speech_mask[ts['start']:ts['end']] = True\n",
    "\n",
    "    # Split into 1 second segments\n",
    "    segment_len = int(sr * segment_length_sec)\n",
    "    total_segments = len(waveform) // segment_len\n",
    "    non_speech_segments = []\n",
    "    if total_segments > no_of_segments: total_segments = no_of_segments\n",
    "    for i in range(total_segments):\n",
    "        start = i * segment_len\n",
    "        end = start + segment_len\n",
    "        segment = waveform[start:end]\n",
    "\n",
    "        # Skip if segment overlaps with speech\n",
    "        if not speech_mask[start:end].any():\n",
    "            non_speech_segments.append(segment)\n",
    "\n",
    "    if not non_speech_segments:\n",
    "      return None, sr\n",
    "    stacked_segments = torch.stack(non_speech_segments)\n",
    "    stacked_segments = stacked_segments.flatten().unsqueeze(0)\n",
    "    return stacked_segments, sr\n",
    "  \n",
    "  def retrieve_embeddings(waveform):\n",
    "     scores, embeddings, spectrogram = yamnet_model(waveform.squeeze(0))\n",
    "     embeddings = torch.from_numpy(embeddings.numpy())\n",
    "     return embeddings\n",
    "\n",
    "  # Resizing all audio files to the same length\n",
    "  def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)\n",
    "  \n",
    "  # Data Augmentation of the Raw Audio Data using Time Shift.\n",
    "  def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "\n",
    "  # Converting raw Audio Data to a Mel Spectrogram\n",
    "  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)\n",
    "  \n",
    "  # Data Augmentation on Mel-Spectrogram Data using Frequency Masks and Time Masks\n",
    "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sound Dataset\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.duration = 3 * 1000\n",
    "        self.sr = 16000\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "                        \n",
    "    # Number of items in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.df)    \n",
    "        \n",
    "    # Get i'th item in dataset\n",
    "    def __getitem__(self, idx):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with the relative path\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_file_path = row['filename']\n",
    "        class_id = row['class']\n",
    "        \n",
    "        audio_file = AudioUtil.read_file(audio_file_path)\n",
    "        aud,sr = AudioUtil.extract_non_speech_segments(audio_file)\n",
    "        if aud is None:\n",
    "            return self.__getitem__((idx + 1) % len(self))  # Skip to next index safely\n",
    "        dur_aud, sr = AudioUtil.pad_trunc(audio_file, self.duration)\n",
    "        embeddings = AudioUtil.retrieve_embeddings(dur_aud)\n",
    "        #shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "        #sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        #aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "        \n",
    "        return embeddings, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2afe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Val Split\n",
    "myds = SoundDS(new_df)\n",
    "val_ds = SoundDS(excluded_df)\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(myds, batch_size=32, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EmbeddingClassifier, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)  # [B, T, 1024] → [B, 1024]\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, 1024]\n",
    "        x = x.permute(0, 2, 1)          # [B, 1024, T]\n",
    "        x = self.pooling(x).squeeze(-1) # [B, 1024]\n",
    "\n",
    "        x = self.fc1(x)                 # [B, 512]\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)                 # [B, 256]\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)                 # [B, num_classes]\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "myModel = EmbeddingClassifier(num_classes=206)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "\n",
    "# Confirm it's on the right device\n",
    "print(next(myModel.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding_model(model, train_dl, num_epochs=25, class_weights=None, optimizer=None, scheduler=None, start_epoch=0, checkpoint_path=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss, optimizer, scheduler\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # Initialize optimizer and scheduler if not provided\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    if scheduler is None:\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=0.001,\n",
    "            steps_per_epoch=len(train_dl),\n",
    "            epochs=num_epochs + start_epoch,\n",
    "            anneal_strategy='linear'\n",
    "        )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for inputs, labels in train_dl:\n",
    "            inputs = inputs.to(device).float()   # [B, T, 1024]\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            # Normalize embeddings (optional but common)\n",
    "            mean, std = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - mean) / std\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)               # [B, num_classes]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dl)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, Time: {time.time()-start_time:.1f}s\")\n",
    "\n",
    "        if checkpoint_path:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict()\n",
    "            }, checkpoint_path)\n",
    "    print(\"✅ Training complete.\")\n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "model, optimizer, scheduler = train_embedding_model(myModel, train_dl, num_epochs=50, class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709276fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51281838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_tensor):\n",
    "    model.eval()\n",
    "    input_tensor = input_tensor.unsqueeze(0).to(device).float()\n",
    "    input_tensor = (input_tensor - input_tensor.mean()) / input_tensor.std()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_class = output.argmax(dim=1).item()\n",
    "        probs = torch.softmax(output, dim=1)  # [1, num_classes]\n",
    "    return predicted_class, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51824ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference (model, val_dl):\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_dl:\n",
    "            val_inputs = val_inputs.to(device).float()\n",
    "            val_labels = val_labels.to(device).long()\n",
    "            val_inputs = (val_inputs - val_inputs.mean()) / val_inputs.std()\n",
    "\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_preds = val_outputs.argmax(dim=1)\n",
    "            val_correct += (val_preds == val_labels).sum().item()\n",
    "            val_total += val_labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        return val_acc\n",
    "\n",
    "# Run inference on trained model with the validation set\n",
    "accuracy = inference(trained_model, val_dl)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".a-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
