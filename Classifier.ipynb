{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed50074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to C:\\Users\\admin/.cache\\torch\\hub\\master.zip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "import random\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import noisereduce as nr\n",
    "# Load the official Silero VAD model\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)\n",
    "(get_speech_timestamps, _, _, _, collect_chunks) = utils\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da15cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "cache_path = os.path.expanduser(\"~\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\")\n",
    "shutil.rmtree(cache_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d597fe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d989a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')\n",
    "class_names =list(pd.read_csv(class_map_path)['display_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1b214d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fb2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df[\"scientific_name\"] = label_encoder.fit_transform(df[\"scientific_name\"])\n",
    "df['filename'] = 'train_audio/' + df['filename'].str[:]\n",
    "df = df[['filename', 'scientific_name']]\n",
    "df.rename(columns={'scientific_name': 'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b2d9a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_31412\\2473492838.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  excluded_df = df.groupby('class').apply(lambda x: x.iloc[5:]).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "new_df = df.groupby('class').head(5).reset_index(drop=True)\n",
    "excluded_df = df.groupby('class').apply(lambda x: x.iloc[5:]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02c297a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = new_df['class'].to_numpy()\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cf8f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Util\n",
    "class AudioUtil():\n",
    "  # Load Audio Data from Source. Returns Tensor and Sample Rate of that Audio File.\n",
    "  def read_file(audio_file_path, target_sr=16000):\n",
    "    waveform, sr = torchaudio.load(audio_file_path)\n",
    "    # converting to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(sr, target_sr) # Converting raw data to target_sr\n",
    "        waveform = resampler(waveform)\n",
    "    denoised_waveform = torch.tensor(nr.reduce_noise(y=waveform, sr=sr))\n",
    "    rms = torch.sqrt(torch.mean(denoised_waveform ** 2))\n",
    "    normalized_waveform =  denoised_waveform * (0.1 / rms) if rms > 0 else waveform\n",
    "    return (normalized_waveform, target_sr)\n",
    "  \n",
    "  def extract_non_speech_segments(audio_file, segment_length_sec=1.0, no_of_segments = 3):\n",
    "    waveform, sr = audio_file\n",
    "    waveform = waveform.squeeze(0)\n",
    "\n",
    "    # Use Silero VAD to get timestamps of speech\n",
    "    speech_timestamps = get_speech_timestamps(waveform, model, sampling_rate=sr)\n",
    "\n",
    "    # Convert speech regions into a mask\n",
    "    speech_mask = torch.zeros_like(waveform, dtype=torch.bool)\n",
    "    for ts in speech_timestamps:\n",
    "        speech_mask[ts['start']:ts['end']] = True\n",
    "\n",
    "    # Split into 1 second segments\n",
    "    segment_len = int(sr * segment_length_sec)\n",
    "    total_segments = len(waveform) // segment_len\n",
    "    non_speech_segments = []\n",
    "    if total_segments > no_of_segments: total_segments = no_of_segments\n",
    "    for i in range(total_segments):\n",
    "        start = i * segment_len\n",
    "        end = start + segment_len\n",
    "        segment = waveform[start:end]\n",
    "\n",
    "        # Skip if segment overlaps with speech\n",
    "        if not speech_mask[start:end].any():\n",
    "            non_speech_segments.append(segment)\n",
    "\n",
    "    if not non_speech_segments:\n",
    "      return None, sr\n",
    "    stacked_segments = torch.stack(non_speech_segments)\n",
    "    stacked_segments = stacked_segments.flatten().unsqueeze(0)\n",
    "    return stacked_segments, sr\n",
    "  \n",
    "  def retrieve_embeddings(waveform):\n",
    "     scores, embeddings, spectrogram = yamnet_model(waveform.squeeze(0))\n",
    "     embeddings = torch.from_numpy(embeddings.numpy())\n",
    "     return embeddings\n",
    "\n",
    "  # Resizing all audio files to the same length\n",
    "  def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)\n",
    "  \n",
    "  # Data Augmentation of the Raw Audio Data using Time Shift.\n",
    "  def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "\n",
    "  # Converting raw Audio Data to a Mel Spectrogram\n",
    "  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)\n",
    "  \n",
    "  # Data Augmentation on Mel-Spectrogram Data using Frequency Masks and Time Masks\n",
    "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "debc448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sound Dataset\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.duration = 3 * 1000\n",
    "        self.sr = 16000\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "                        \n",
    "    # Number of items in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.df)    \n",
    "        \n",
    "    # Get i'th item in dataset\n",
    "    def __getitem__(self, idx):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with the relative path\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_file_path = row['filename']\n",
    "        class_id = row['class']\n",
    "        \n",
    "        audio_file = AudioUtil.read_file(audio_file_path)\n",
    "        aud,sr = AudioUtil.extract_non_speech_segments(audio_file)\n",
    "        if aud is None:\n",
    "            return self.__getitem__((idx + 1) % len(self))  # Skip to next index safely\n",
    "        dur_aud, sr = AudioUtil.pad_trunc(audio_file, self.duration)\n",
    "        embeddings = AudioUtil.retrieve_embeddings(dur_aud)\n",
    "        #shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "        #sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        #aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "        \n",
    "        return embeddings, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a2afe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Val Split\n",
    "myds = SoundDS(new_df)\n",
    "val_ds = SoundDS(excluded_df)\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(myds, batch_size=32, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc46c450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EmbeddingClassifier, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)  # [B, T, 1024] â†’ [B, 1024]\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, 1024]\n",
    "        x = x.permute(0, 2, 1)          # [B, 1024, T]\n",
    "        x = self.pooling(x).squeeze(-1) # [B, 1024]\n",
    "\n",
    "        x = self.fc1(x)                 # [B, 512]\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)                 # [B, 256]\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)                 # [B, num_classes]\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "myModel = EmbeddingClassifier(num_classes=206)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "\n",
    "# Confirm it's on the right device\n",
    "print(next(myModel.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5691adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding_model(model, train_dl, num_epochs=25, class_weights=None, optimizer=None, scheduler=None, start_epoch=0, checkpoint_path=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss, optimizer, scheduler\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # Initialize optimizer and scheduler if not provided\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    if scheduler is None:\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=0.001,\n",
    "            steps_per_epoch=len(train_dl),\n",
    "            epochs=num_epochs + start_epoch,\n",
    "            anneal_strategy='linear'\n",
    "        )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for inputs, labels in train_dl:\n",
    "            inputs = inputs.to(device).float()   # [B, T, 1024]\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            # Normalize embeddings (optional but common)\n",
    "            mean, std = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - mean) / std\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)               # [B, num_classes]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dl)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, Time: {time.time()-start_time:.1f}s\")\n",
    "\n",
    "        if checkpoint_path:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict()\n",
    "            }, checkpoint_path)\n",
    "    print(\"Training complete.\")\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6246b44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1] Train Loss: 5.3720, Accuracy: 0.0123, Time: 1192.8s\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, scheduler = train_embedding_model(myModel, train_dl, num_epochs=1, class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_weights.pt\")\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"final_scripted_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51824ff2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EmbeddingClassifier' object has no attribute 'reset_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m val_acc\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Run inference on trained model with the validation set\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy)\n",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m, in \u001b[0;36minference\u001b[1;34m(model, val_dl)\u001b[0m\n\u001b[0;32m      4\u001b[0m val_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val_inputs, val_labels \u001b[38;5;129;01min\u001b[39;00m val_dl:\n\u001b[0;32m      7\u001b[0m         val_inputs \u001b[38;5;241m=\u001b[39m val_inputs\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m      8\u001b[0m         val_labels \u001b[38;5;241m=\u001b[39m val_labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m, in \u001b[0;36mSoundDS.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m class_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     21\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m AudioUtil\u001b[38;5;241m.\u001b[39mread_file(audio_file_path)\n\u001b[1;32m---> 22\u001b[0m aud,sr \u001b[38;5;241m=\u001b[39m \u001b[43mAudioUtil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_non_speech_segments\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aud \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m((idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))  \u001b[38;5;66;03m# Skip to next index safely\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m, in \u001b[0;36mAudioUtil.extract_non_speech_segments\u001b[1;34m(audio_file, segment_length_sec, no_of_segments)\u001b[0m\n\u001b[0;32m     19\u001b[0m waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Use Silero VAD to get timestamps of speech\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m speech_timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mget_speech_timestamps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Convert speech regions into a mask\u001b[39;00m\n\u001b[0;32m     25\u001b[0m speech_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(waveform, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\snakers4_silero-vad_master\\src\\silero_vad\\utils_vad.py:286\u001b[0m, in \u001b[0;36mget_speech_timestamps\u001b[1;34m(audio, model, threshold, sampling_rate, min_speech_duration_ms, max_speech_duration_s, min_silence_duration_ms, speech_pad_ms, return_seconds, time_resolution, visualize_probs, progress_tracking_callback, neg_threshold, window_size_samples)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently silero VAD models support 8000 and 16000 (or multiply of 16000) sample rates\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    284\u001b[0m window_size_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sampling_rate \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m16000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m--> 286\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_states\u001b[49m()\n\u001b[0;32m    287\u001b[0m min_speech_samples \u001b[38;5;241m=\u001b[39m sampling_rate \u001b[38;5;241m*\u001b[39m min_speech_duration_ms \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m    288\u001b[0m speech_pad_samples \u001b[38;5;241m=\u001b[39m sampling_rate \u001b[38;5;241m*\u001b[39m speech_pad_ms \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1942\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EmbeddingClassifier' object has no attribute 'reset_states'"
     ]
    }
   ],
   "source": [
    "def inference(model, val_dl, scripted=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if scripted:\n",
    "        model = torch.jit.load(model)  # if you passed a path\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_dl:\n",
    "            val_inputs = val_inputs.to(device).float()\n",
    "            val_labels = val_labels.to(device).long()\n",
    "\n",
    "            val_inputs = (val_inputs - val_inputs.mean()) / val_inputs.std()\n",
    "\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_preds = val_outputs.argmax(dim=1)\n",
    "            val_correct += (val_preds == val_labels).sum().item()\n",
    "            val_total += val_labels.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total if val_total > 0 else 0.0\n",
    "    print(f\"Inference Accuracy: {val_acc:.4f}\")\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c7045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler = train_embedding_model(myModel, train_dl, num_epochs=1, class_weights=class_weights, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51281838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_tensor):\n",
    "    model.eval()\n",
    "    input_tensor = input_tensor.unsqueeze(0).to(device).float()\n",
    "    input_tensor = (input_tensor - input_tensor.mean()) / input_tensor.std()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_class = output.argmax(dim=1).item()\n",
    "        probs = torch.softmax(output, dim=1)  # [1, num_classes]\n",
    "    return predicted_class, probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".a-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
