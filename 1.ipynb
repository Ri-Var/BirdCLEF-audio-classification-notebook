{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed50074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to C:\\Users\\admin/.cache\\torch\\hub\\master.zip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "import random\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import noisereduce as nr\n",
    "# Load the official Silero VAD model\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)\n",
    "(get_speech_timestamps, _, _, _, collect_chunks) = utils\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d597fe5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trying to load a model of incompatible/unknown type. 'C:\\Users\\admin\\AppData\\Local\\Temp\\tfhub_modules\\9616fd04ec2360621642ef9455b84f4b668e219e' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m yamnet_model_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://tfhub.dev/google/yamnet/1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m yamnet_model \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43myamnet_model_handle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\tensorflow_hub\\module_v2.py:113\u001b[0m, in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    108\u001b[0m saved_model_pbtxt_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    109\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_bytes(module_path),\n\u001b[0;32m    110\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_bytes(tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(saved_model_path) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(saved_model_pbtxt_path)):\n\u001b[1;32m--> 113\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load a model of incompatible/unknown type. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    114\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m contains neither \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m nor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    115\u001b[0m                    (module_path, tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PB,\n\u001b[0;32m    116\u001b[0m                     tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options:\n\u001b[0;32m    119\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(tf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoadOptions\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Trying to load a model of incompatible/unknown type. 'C:\\Users\\admin\\AppData\\Local\\Temp\\tfhub_modules\\9616fd04ec2360621642ef9455b84f4b668e219e' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'."
     ]
    }
   ],
   "source": [
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d989a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Speech',\n",
       " 'Child speech, kid speaking',\n",
       " 'Conversation',\n",
       " 'Narration, monologue',\n",
       " 'Babbling',\n",
       " 'Speech synthesizer',\n",
       " 'Shout',\n",
       " 'Bellow',\n",
       " 'Whoop',\n",
       " 'Yell',\n",
       " 'Children shouting',\n",
       " 'Screaming',\n",
       " 'Whispering',\n",
       " 'Laughter',\n",
       " 'Baby laughter',\n",
       " 'Giggle',\n",
       " 'Snicker',\n",
       " 'Belly laugh',\n",
       " 'Chuckle, chortle',\n",
       " 'Crying, sobbing',\n",
       " 'Baby cry, infant cry',\n",
       " 'Whimper',\n",
       " 'Wail, moan',\n",
       " 'Sigh',\n",
       " 'Singing',\n",
       " 'Choir',\n",
       " 'Yodeling',\n",
       " 'Chant',\n",
       " 'Mantra',\n",
       " 'Child singing',\n",
       " 'Synthetic singing',\n",
       " 'Rapping',\n",
       " 'Humming',\n",
       " 'Groan',\n",
       " 'Grunt',\n",
       " 'Whistling',\n",
       " 'Breathing',\n",
       " 'Wheeze',\n",
       " 'Snoring',\n",
       " 'Gasp',\n",
       " 'Pant',\n",
       " 'Snort',\n",
       " 'Cough',\n",
       " 'Throat clearing',\n",
       " 'Sneeze',\n",
       " 'Sniff',\n",
       " 'Run',\n",
       " 'Shuffle',\n",
       " 'Walk, footsteps',\n",
       " 'Chewing, mastication',\n",
       " 'Biting',\n",
       " 'Gargling',\n",
       " 'Stomach rumble',\n",
       " 'Burping, eructation',\n",
       " 'Hiccup',\n",
       " 'Fart',\n",
       " 'Hands',\n",
       " 'Finger snapping',\n",
       " 'Clapping',\n",
       " 'Heart sounds, heartbeat',\n",
       " 'Heart murmur',\n",
       " 'Cheering',\n",
       " 'Applause',\n",
       " 'Chatter',\n",
       " 'Crowd',\n",
       " 'Hubbub, speech noise, speech babble',\n",
       " 'Children playing',\n",
       " 'Animal',\n",
       " 'Domestic animals, pets',\n",
       " 'Dog',\n",
       " 'Bark',\n",
       " 'Yip',\n",
       " 'Howl',\n",
       " 'Bow-wow',\n",
       " 'Growling',\n",
       " 'Whimper (dog)',\n",
       " 'Cat',\n",
       " 'Purr',\n",
       " 'Meow',\n",
       " 'Hiss',\n",
       " 'Caterwaul',\n",
       " 'Livestock, farm animals, working animals',\n",
       " 'Horse',\n",
       " 'Clip-clop',\n",
       " 'Neigh, whinny',\n",
       " 'Cattle, bovinae',\n",
       " 'Moo',\n",
       " 'Cowbell',\n",
       " 'Pig',\n",
       " 'Oink',\n",
       " 'Goat',\n",
       " 'Bleat',\n",
       " 'Sheep',\n",
       " 'Fowl',\n",
       " 'Chicken, rooster',\n",
       " 'Cluck',\n",
       " 'Crowing, cock-a-doodle-doo',\n",
       " 'Turkey',\n",
       " 'Gobble',\n",
       " 'Duck',\n",
       " 'Quack',\n",
       " 'Goose',\n",
       " 'Honk',\n",
       " 'Wild animals',\n",
       " 'Roaring cats (lions, tigers)',\n",
       " 'Roar',\n",
       " 'Bird',\n",
       " 'Bird vocalization, bird call, bird song',\n",
       " 'Chirp, tweet',\n",
       " 'Squawk',\n",
       " 'Pigeon, dove',\n",
       " 'Coo',\n",
       " 'Crow',\n",
       " 'Caw',\n",
       " 'Owl',\n",
       " 'Hoot',\n",
       " 'Bird flight, flapping wings',\n",
       " 'Canidae, dogs, wolves',\n",
       " 'Rodents, rats, mice',\n",
       " 'Mouse',\n",
       " 'Patter',\n",
       " 'Insect',\n",
       " 'Cricket',\n",
       " 'Mosquito',\n",
       " 'Fly, housefly',\n",
       " 'Buzz',\n",
       " 'Bee, wasp, etc.',\n",
       " 'Frog',\n",
       " 'Croak',\n",
       " 'Snake',\n",
       " 'Rattle',\n",
       " 'Whale vocalization',\n",
       " 'Music',\n",
       " 'Musical instrument',\n",
       " 'Plucked string instrument',\n",
       " 'Guitar',\n",
       " 'Electric guitar',\n",
       " 'Bass guitar',\n",
       " 'Acoustic guitar',\n",
       " 'Steel guitar, slide guitar',\n",
       " 'Tapping (guitar technique)',\n",
       " 'Strum',\n",
       " 'Banjo',\n",
       " 'Sitar',\n",
       " 'Mandolin',\n",
       " 'Zither',\n",
       " 'Ukulele',\n",
       " 'Keyboard (musical)',\n",
       " 'Piano',\n",
       " 'Electric piano',\n",
       " 'Organ',\n",
       " 'Electronic organ',\n",
       " 'Hammond organ',\n",
       " 'Synthesizer',\n",
       " 'Sampler',\n",
       " 'Harpsichord',\n",
       " 'Percussion',\n",
       " 'Drum kit',\n",
       " 'Drum machine',\n",
       " 'Drum',\n",
       " 'Snare drum',\n",
       " 'Rimshot',\n",
       " 'Drum roll',\n",
       " 'Bass drum',\n",
       " 'Timpani',\n",
       " 'Tabla',\n",
       " 'Cymbal',\n",
       " 'Hi-hat',\n",
       " 'Wood block',\n",
       " 'Tambourine',\n",
       " 'Rattle (instrument)',\n",
       " 'Maraca',\n",
       " 'Gong',\n",
       " 'Tubular bells',\n",
       " 'Mallet percussion',\n",
       " 'Marimba, xylophone',\n",
       " 'Glockenspiel',\n",
       " 'Vibraphone',\n",
       " 'Steelpan',\n",
       " 'Orchestra',\n",
       " 'Brass instrument',\n",
       " 'French horn',\n",
       " 'Trumpet',\n",
       " 'Trombone',\n",
       " 'Bowed string instrument',\n",
       " 'String section',\n",
       " 'Violin, fiddle',\n",
       " 'Pizzicato',\n",
       " 'Cello',\n",
       " 'Double bass',\n",
       " 'Wind instrument, woodwind instrument',\n",
       " 'Flute',\n",
       " 'Saxophone',\n",
       " 'Clarinet',\n",
       " 'Harp',\n",
       " 'Bell',\n",
       " 'Church bell',\n",
       " 'Jingle bell',\n",
       " 'Bicycle bell',\n",
       " 'Tuning fork',\n",
       " 'Chime',\n",
       " 'Wind chime',\n",
       " 'Change ringing (campanology)',\n",
       " 'Harmonica',\n",
       " 'Accordion',\n",
       " 'Bagpipes',\n",
       " 'Didgeridoo',\n",
       " 'Shofar',\n",
       " 'Theremin',\n",
       " 'Singing bowl',\n",
       " 'Scratching (performance technique)',\n",
       " 'Pop music',\n",
       " 'Hip hop music',\n",
       " 'Beatboxing',\n",
       " 'Rock music',\n",
       " 'Heavy metal',\n",
       " 'Punk rock',\n",
       " 'Grunge',\n",
       " 'Progressive rock',\n",
       " 'Rock and roll',\n",
       " 'Psychedelic rock',\n",
       " 'Rhythm and blues',\n",
       " 'Soul music',\n",
       " 'Reggae',\n",
       " 'Country',\n",
       " 'Swing music',\n",
       " 'Bluegrass',\n",
       " 'Funk',\n",
       " 'Folk music',\n",
       " 'Middle Eastern music',\n",
       " 'Jazz',\n",
       " 'Disco',\n",
       " 'Classical music',\n",
       " 'Opera',\n",
       " 'Electronic music',\n",
       " 'House music',\n",
       " 'Techno',\n",
       " 'Dubstep',\n",
       " 'Drum and bass',\n",
       " 'Electronica',\n",
       " 'Electronic dance music',\n",
       " 'Ambient music',\n",
       " 'Trance music',\n",
       " 'Music of Latin America',\n",
       " 'Salsa music',\n",
       " 'Flamenco',\n",
       " 'Blues',\n",
       " 'Music for children',\n",
       " 'New-age music',\n",
       " 'Vocal music',\n",
       " 'A capella',\n",
       " 'Music of Africa',\n",
       " 'Afrobeat',\n",
       " 'Christian music',\n",
       " 'Gospel music',\n",
       " 'Music of Asia',\n",
       " 'Carnatic music',\n",
       " 'Music of Bollywood',\n",
       " 'Ska',\n",
       " 'Traditional music',\n",
       " 'Independent music',\n",
       " 'Song',\n",
       " 'Background music',\n",
       " 'Theme music',\n",
       " 'Jingle (music)',\n",
       " 'Soundtrack music',\n",
       " 'Lullaby',\n",
       " 'Video game music',\n",
       " 'Christmas music',\n",
       " 'Dance music',\n",
       " 'Wedding music',\n",
       " 'Happy music',\n",
       " 'Sad music',\n",
       " 'Tender music',\n",
       " 'Exciting music',\n",
       " 'Angry music',\n",
       " 'Scary music',\n",
       " 'Wind',\n",
       " 'Rustling leaves',\n",
       " 'Wind noise (microphone)',\n",
       " 'Thunderstorm',\n",
       " 'Thunder',\n",
       " 'Water',\n",
       " 'Rain',\n",
       " 'Raindrop',\n",
       " 'Rain on surface',\n",
       " 'Stream',\n",
       " 'Waterfall',\n",
       " 'Ocean',\n",
       " 'Waves, surf',\n",
       " 'Steam',\n",
       " 'Gurgling',\n",
       " 'Fire',\n",
       " 'Crackle',\n",
       " 'Vehicle',\n",
       " 'Boat, Water vehicle',\n",
       " 'Sailboat, sailing ship',\n",
       " 'Rowboat, canoe, kayak',\n",
       " 'Motorboat, speedboat',\n",
       " 'Ship',\n",
       " 'Motor vehicle (road)',\n",
       " 'Car',\n",
       " 'Vehicle horn, car horn, honking',\n",
       " 'Toot',\n",
       " 'Car alarm',\n",
       " 'Power windows, electric windows',\n",
       " 'Skidding',\n",
       " 'Tire squeal',\n",
       " 'Car passing by',\n",
       " 'Race car, auto racing',\n",
       " 'Truck',\n",
       " 'Air brake',\n",
       " 'Air horn, truck horn',\n",
       " 'Reversing beeps',\n",
       " 'Ice cream truck, ice cream van',\n",
       " 'Bus',\n",
       " 'Emergency vehicle',\n",
       " 'Police car (siren)',\n",
       " 'Ambulance (siren)',\n",
       " 'Fire engine, fire truck (siren)',\n",
       " 'Motorcycle',\n",
       " 'Traffic noise, roadway noise',\n",
       " 'Rail transport',\n",
       " 'Train',\n",
       " 'Train whistle',\n",
       " 'Train horn',\n",
       " 'Railroad car, train wagon',\n",
       " 'Train wheels squealing',\n",
       " 'Subway, metro, underground',\n",
       " 'Aircraft',\n",
       " 'Aircraft engine',\n",
       " 'Jet engine',\n",
       " 'Propeller, airscrew',\n",
       " 'Helicopter',\n",
       " 'Fixed-wing aircraft, airplane',\n",
       " 'Bicycle',\n",
       " 'Skateboard',\n",
       " 'Engine',\n",
       " 'Light engine (high frequency)',\n",
       " \"Dental drill, dentist's drill\",\n",
       " 'Lawn mower',\n",
       " 'Chainsaw',\n",
       " 'Medium engine (mid frequency)',\n",
       " 'Heavy engine (low frequency)',\n",
       " 'Engine knocking',\n",
       " 'Engine starting',\n",
       " 'Idling',\n",
       " 'Accelerating, revving, vroom',\n",
       " 'Door',\n",
       " 'Doorbell',\n",
       " 'Ding-dong',\n",
       " 'Sliding door',\n",
       " 'Slam',\n",
       " 'Knock',\n",
       " 'Tap',\n",
       " 'Squeak',\n",
       " 'Cupboard open or close',\n",
       " 'Drawer open or close',\n",
       " 'Dishes, pots, and pans',\n",
       " 'Cutlery, silverware',\n",
       " 'Chopping (food)',\n",
       " 'Frying (food)',\n",
       " 'Microwave oven',\n",
       " 'Blender',\n",
       " 'Water tap, faucet',\n",
       " 'Sink (filling or washing)',\n",
       " 'Bathtub (filling or washing)',\n",
       " 'Hair dryer',\n",
       " 'Toilet flush',\n",
       " 'Toothbrush',\n",
       " 'Electric toothbrush',\n",
       " 'Vacuum cleaner',\n",
       " 'Zipper (clothing)',\n",
       " 'Keys jangling',\n",
       " 'Coin (dropping)',\n",
       " 'Scissors',\n",
       " 'Electric shaver, electric razor',\n",
       " 'Shuffling cards',\n",
       " 'Typing',\n",
       " 'Typewriter',\n",
       " 'Computer keyboard',\n",
       " 'Writing',\n",
       " 'Alarm',\n",
       " 'Telephone',\n",
       " 'Telephone bell ringing',\n",
       " 'Ringtone',\n",
       " 'Telephone dialing, DTMF',\n",
       " 'Dial tone',\n",
       " 'Busy signal',\n",
       " 'Alarm clock',\n",
       " 'Siren',\n",
       " 'Civil defense siren',\n",
       " 'Buzzer',\n",
       " 'Smoke detector, smoke alarm',\n",
       " 'Fire alarm',\n",
       " 'Foghorn',\n",
       " 'Whistle',\n",
       " 'Steam whistle',\n",
       " 'Mechanisms',\n",
       " 'Ratchet, pawl',\n",
       " 'Clock',\n",
       " 'Tick',\n",
       " 'Tick-tock',\n",
       " 'Gears',\n",
       " 'Pulleys',\n",
       " 'Sewing machine',\n",
       " 'Mechanical fan',\n",
       " 'Air conditioning',\n",
       " 'Cash register',\n",
       " 'Printer',\n",
       " 'Camera',\n",
       " 'Single-lens reflex camera',\n",
       " 'Tools',\n",
       " 'Hammer',\n",
       " 'Jackhammer',\n",
       " 'Sawing',\n",
       " 'Filing (rasp)',\n",
       " 'Sanding',\n",
       " 'Power tool',\n",
       " 'Drill',\n",
       " 'Explosion',\n",
       " 'Gunshot, gunfire',\n",
       " 'Machine gun',\n",
       " 'Fusillade',\n",
       " 'Artillery fire',\n",
       " 'Cap gun',\n",
       " 'Fireworks',\n",
       " 'Firecracker',\n",
       " 'Burst, pop',\n",
       " 'Eruption',\n",
       " 'Boom',\n",
       " 'Wood',\n",
       " 'Chop',\n",
       " 'Splinter',\n",
       " 'Crack',\n",
       " 'Glass',\n",
       " 'Chink, clink',\n",
       " 'Shatter',\n",
       " 'Liquid',\n",
       " 'Splash, splatter',\n",
       " 'Slosh',\n",
       " 'Squish',\n",
       " 'Drip',\n",
       " 'Pour',\n",
       " 'Trickle, dribble',\n",
       " 'Gush',\n",
       " 'Fill (with liquid)',\n",
       " 'Spray',\n",
       " 'Pump (liquid)',\n",
       " 'Stir',\n",
       " 'Boiling',\n",
       " 'Sonar',\n",
       " 'Arrow',\n",
       " 'Whoosh, swoosh, swish',\n",
       " 'Thump, thud',\n",
       " 'Thunk',\n",
       " 'Electronic tuner',\n",
       " 'Effects unit',\n",
       " 'Chorus effect',\n",
       " 'Basketball bounce',\n",
       " 'Bang',\n",
       " 'Slap, smack',\n",
       " 'Whack, thwack',\n",
       " 'Smash, crash',\n",
       " 'Breaking',\n",
       " 'Bouncing',\n",
       " 'Whip',\n",
       " 'Flap',\n",
       " 'Scratch',\n",
       " 'Scrape',\n",
       " 'Rub',\n",
       " 'Roll',\n",
       " 'Crushing',\n",
       " 'Crumpling, crinkling',\n",
       " 'Tearing',\n",
       " 'Beep, bleep',\n",
       " 'Ping',\n",
       " 'Ding',\n",
       " 'Clang',\n",
       " 'Squeal',\n",
       " 'Creak',\n",
       " 'Rustle',\n",
       " 'Whir',\n",
       " 'Clatter',\n",
       " 'Sizzle',\n",
       " 'Clicking',\n",
       " 'Clickety-clack',\n",
       " 'Rumble',\n",
       " 'Plop',\n",
       " 'Jingle, tinkle',\n",
       " 'Hum',\n",
       " 'Zing',\n",
       " 'Boing',\n",
       " 'Crunch',\n",
       " 'Silence',\n",
       " 'Sine wave',\n",
       " 'Harmonic',\n",
       " 'Chirp tone',\n",
       " 'Sound effect',\n",
       " 'Pulse',\n",
       " 'Inside, small room',\n",
       " 'Inside, large room or hall',\n",
       " 'Inside, public space',\n",
       " 'Outside, urban or manmade',\n",
       " 'Outside, rural or natural',\n",
       " 'Reverberation',\n",
       " 'Echo',\n",
       " 'Noise',\n",
       " 'Environmental noise',\n",
       " 'Static',\n",
       " 'Mains hum',\n",
       " 'Distortion',\n",
       " 'Sidetone',\n",
       " 'Cacophony',\n",
       " 'White noise',\n",
       " 'Pink noise',\n",
       " 'Throbbing',\n",
       " 'Vibration',\n",
       " 'Television',\n",
       " 'Radio',\n",
       " 'Field recording']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')\n",
    "class_names =list(pd.read_csv(class_map_path)['display_name'])\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b214d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df[\"scientific_name\"] = label_encoder.fit_transform(df[\"scientific_name\"])\n",
    "df['filename'] = 'train_audio/' + df['filename'].str[:]\n",
    "df = df[['filename', 'scientific_name']]\n",
    "df.rename(columns={'scientific_name': 'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d9a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20112\\2473492838.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  excluded_df = df.groupby('class').apply(lambda x: x.iloc[5:]).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "new_df = df.groupby('class').head(5).reset_index(drop=True)\n",
    "excluded_df = df.groupby('class').apply(lambda x: x.iloc[5:]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c297a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = new_df['class'].to_numpy()\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf8f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Util\n",
    "class AudioUtil():\n",
    "  # Load Audio Data from Source. Returns Tensor and Sample Rate of that Audio File.\n",
    "  def read_file(audio_file_path, target_sr=16000):\n",
    "    waveform, sr = torchaudio.load(audio_file_path)\n",
    "    # converting to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(sr, target_sr) # Converting raw data to target_sr\n",
    "        waveform = resampler(waveform)\n",
    "    denoised_waveform = torch.tensor(nr.reduce_noise(y=waveform, sr=sr))\n",
    "    rms = torch.sqrt(torch.mean(denoised_waveform ** 2))\n",
    "    normalized_waveform =  denoised_waveform * (0.1 / rms) if rms > 0 else waveform\n",
    "    return (normalized_waveform, target_sr)\n",
    "  \n",
    "  def extract_non_speech_segments(audio_file, segment_length_sec=1.0, no_of_segments = 3):\n",
    "    waveform, sr = audio_file\n",
    "    waveform = waveform.squeeze(0)\n",
    "\n",
    "    # Use Silero VAD to get timestamps of speech\n",
    "    speech_timestamps = get_speech_timestamps(waveform, model, sampling_rate=sr)\n",
    "\n",
    "    # Convert speech regions into a mask\n",
    "    speech_mask = torch.zeros_like(waveform, dtype=torch.bool)\n",
    "    for ts in speech_timestamps:\n",
    "        speech_mask[ts['start']:ts['end']] = True\n",
    "\n",
    "    # Split into 1 second segments\n",
    "    segment_len = int(sr * segment_length_sec)\n",
    "    total_segments = len(waveform) // segment_len\n",
    "    non_speech_segments = []\n",
    "    if total_segments > no_of_segments: total_segments = no_of_segments\n",
    "    for i in range(total_segments):\n",
    "        start = i * segment_len\n",
    "        end = start + segment_len\n",
    "        segment = waveform[start:end]\n",
    "\n",
    "        # Skip if segment overlaps with speech\n",
    "        if not speech_mask[start:end].any():\n",
    "            non_speech_segments.append(segment)\n",
    "\n",
    "    if not non_speech_segments:\n",
    "      return None, sr\n",
    "    stacked_segments = torch.stack(non_speech_segments)\n",
    "    stacked_segments = stacked_segments.flatten().unsqueeze(0)\n",
    "    return stacked_segments, sr\n",
    "  \n",
    "  def retrieve_embeddings(waveform):\n",
    "     scores, embeddings, spectrogram = yamnet_model(waveform.squeeze(0))\n",
    "     embeddings = torch.from_numpy(embeddings.numpy())\n",
    "     return embeddings\n",
    "\n",
    "  # Resizing all audio files to the same length\n",
    "  def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)\n",
    "  \n",
    "  # Data Augmentation of the Raw Audio Data using Time Shift.\n",
    "  def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "\n",
    "  # Converting raw Audio Data to a Mel Spectrogram\n",
    "  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)\n",
    "  \n",
    "  # Data Augmentation on Mel-Spectrogram Data using Frequency Masks and Time Masks\n",
    "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sound Dataset\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.duration = 3 * 1000\n",
    "        self.sr = 16000\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "                        \n",
    "    # Number of items in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.df)    \n",
    "        \n",
    "    # Get i'th item in dataset\n",
    "    def __getitem__(self, idx):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with the relative path\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_file_path = row['filename']\n",
    "        class_id = row['class']\n",
    "        \n",
    "        audio_file = AudioUtil.read_file(audio_file_path)\n",
    "        aud,sr = AudioUtil.extract_non_speech_segments(audio_file)\n",
    "        if aud is None:\n",
    "            return self.__getitem__((idx + 1) % len(self))  # Skip to next index safely\n",
    "        dur_aud, sr = AudioUtil.pad_trunc(audio_file, self.duration)\n",
    "        embeddings = AudioUtil.retrieve_embeddings(dur_aud)\n",
    "        #shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "        #sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        #aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "        \n",
    "        return embeddings, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2afe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Val Split\n",
    "myds = SoundDS(new_df)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "\"\"\"num_items = len(myds)\n",
    "num_train = round(num_items * 1)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\"\"\"\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(myds, batch_size=32, shuffle=True)\n",
    "#val_dl = torch.utils.data.DataLoader(val_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46c450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EmbeddingClassifier, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)  # [B, T, 1024] → [B, 1024]\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, 1024]\n",
    "        x = x.permute(0, 2, 1)          # [B, 1024, T]\n",
    "        x = self.pooling(x).squeeze(-1) # [B, 1024]\n",
    "\n",
    "        x = self.fc1(x)                 # [B, 512]\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)                 # [B, 256]\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)                 # [B, num_classes]\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "myModel = EmbeddingClassifier(num_classes=206)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "\n",
    "# Confirm it's on the right device\n",
    "print(next(myModel.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691adea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] Train Loss: 5.3304, Accuracy: 0.0031, Time: 1096.4s\n",
      "[Epoch 2/50] Train Loss: 5.2954, Accuracy: 0.0041, Time: 1094.5s\n",
      "[Epoch 3/50] Train Loss: 5.2326, Accuracy: 0.0206, Time: 1092.6s\n",
      "[Epoch 4/50] Train Loss: 5.1837, Accuracy: 0.0216, Time: 1093.0s\n",
      "[Epoch 5/50] Train Loss: 5.1526, Accuracy: 0.0236, Time: 1364.1s\n",
      "[Epoch 6/50] Train Loss: 5.0306, Accuracy: 0.0298, Time: 2280.1s\n",
      "[Epoch 7/50] Train Loss: 4.9438, Accuracy: 0.0349, Time: 1829.2s\n",
      "[Epoch 8/50] Train Loss: 4.8028, Accuracy: 0.0380, Time: 1293.2s\n",
      "[Epoch 9/50] Train Loss: 4.6434, Accuracy: 0.0606, Time: 1145.0s\n",
      "[Epoch 10/50] Train Loss: 4.4613, Accuracy: 0.0658, Time: 1115.5s\n",
      "[Epoch 11/50] Train Loss: 4.3170, Accuracy: 0.0894, Time: 1134.2s\n",
      "[Epoch 12/50] Train Loss: 4.1617, Accuracy: 0.1079, Time: 1111.5s\n",
      "[Epoch 13/50] Train Loss: 4.0013, Accuracy: 0.1202, Time: 1107.8s\n",
      "[Epoch 14/50] Train Loss: 3.8867, Accuracy: 0.1202, Time: 1116.5s\n",
      "[Epoch 15/50] Train Loss: 3.7003, Accuracy: 0.1675, Time: 890.6s\n",
      "[Epoch 16/50] Train Loss: 3.6452, Accuracy: 0.1614, Time: 862.3s\n",
      "[Epoch 17/50] Train Loss: 3.3829, Accuracy: 0.2035, Time: 863.1s\n",
      "[Epoch 18/50] Train Loss: 3.1876, Accuracy: 0.2467, Time: 1051.5s\n",
      "[Epoch 19/50] Train Loss: 3.0190, Accuracy: 0.2395, Time: 1095.0s\n",
      "[Epoch 20/50] Train Loss: 2.8730, Accuracy: 0.2867, Time: 1092.4s\n",
      "[Epoch 21/50] Train Loss: 2.7354, Accuracy: 0.3237, Time: 1106.9s\n",
      "[Epoch 22/50] Train Loss: 2.5959, Accuracy: 0.3402, Time: 1096.9s\n",
      "[Epoch 23/50] Train Loss: 2.4728, Accuracy: 0.3535, Time: 1070.2s\n",
      "[Epoch 24/50] Train Loss: 2.3632, Accuracy: 0.3782, Time: 1024.0s\n",
      "[Epoch 25/50] Train Loss: 2.1601, Accuracy: 0.4203, Time: 1145.5s\n",
      "[Epoch 26/50] Train Loss: 2.1145, Accuracy: 0.4388, Time: 1095.9s\n",
      "[Epoch 27/50] Train Loss: 1.9785, Accuracy: 0.4676, Time: 1075.7s\n",
      "[Epoch 28/50] Train Loss: 1.8037, Accuracy: 0.5180, Time: 1040.0s\n",
      "[Epoch 29/50] Train Loss: 1.7659, Accuracy: 0.5170, Time: 1029.5s\n",
      "[Epoch 30/50] Train Loss: 1.6510, Accuracy: 0.5324, Time: 1040.2s\n",
      "[Epoch 31/50] Train Loss: 1.5581, Accuracy: 0.5745, Time: 1054.6s\n",
      "[Epoch 32/50] Train Loss: 1.4346, Accuracy: 0.6012, Time: 1025.2s\n",
      "[Epoch 33/50] Train Loss: 1.4207, Accuracy: 0.6187, Time: 1025.1s\n",
      "[Epoch 34/50] Train Loss: 1.3656, Accuracy: 0.6197, Time: 1020.3s\n",
      "[Epoch 35/50] Train Loss: 1.2780, Accuracy: 0.6567, Time: 1033.4s\n",
      "[Epoch 36/50] Train Loss: 1.2448, Accuracy: 0.6526, Time: 1022.4s\n",
      "[Epoch 37/50] Train Loss: 1.1087, Accuracy: 0.6948, Time: 1025.3s\n",
      "[Epoch 38/50] Train Loss: 1.1092, Accuracy: 0.6845, Time: 1037.7s\n",
      "[Epoch 39/50] Train Loss: 1.0871, Accuracy: 0.7102, Time: 1026.5s\n",
      "[Epoch 40/50] Train Loss: 1.0564, Accuracy: 0.6896, Time: 1034.0s\n",
      "[Epoch 41/50] Train Loss: 0.9891, Accuracy: 0.7338, Time: 1021.1s\n",
      "[Epoch 42/50] Train Loss: 0.9922, Accuracy: 0.7318, Time: 9072.7s\n",
      "[Epoch 43/50] Train Loss: 0.8992, Accuracy: 0.7523, Time: 1090.0s\n",
      "[Epoch 44/50] Train Loss: 0.9055, Accuracy: 0.7544, Time: 1140.0s\n",
      "[Epoch 45/50] Train Loss: 0.8904, Accuracy: 0.7585, Time: 1077.3s\n",
      "[Epoch 46/50] Train Loss: 0.8733, Accuracy: 0.7657, Time: 1667.1s\n",
      "[Epoch 47/50] Train Loss: 0.8769, Accuracy: 0.7677, Time: 1416.2s\n",
      "[Epoch 48/50] Train Loss: 0.8460, Accuracy: 0.7780, Time: 1121.8s\n",
      "[Epoch 49/50] Train Loss: 0.7829, Accuracy: 0.7975, Time: 1178.3s\n",
      "[Epoch 50/50] Train Loss: 0.8091, Accuracy: 0.7708, Time: 1103.3s\n",
      "✅ Training complete.\n"
     ]
    }
   ],
   "source": [
    "def train_embedding_model(model, train_dl, num_epochs=25, class_weights=None, optimizer=None, scheduler=None, start_epoch=0, checkpoint_path=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss, optimizer, scheduler\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # Initialize optimizer and scheduler if not provided\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    if scheduler is None:\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=0.001,\n",
    "            steps_per_epoch=len(train_dl),\n",
    "            epochs=num_epochs + start_epoch,\n",
    "            anneal_strategy='linear'\n",
    "        )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for inputs, labels in train_dl:\n",
    "            inputs = inputs.to(device).float()   # [B, T, 1024]\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            # Normalize embeddings (optional but common)\n",
    "            mean, std = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - mean) / std\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)               # [B, num_classes]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dl)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, Time: {time.time()-start_time:.1f}s\")\n",
    "\n",
    "        if checkpoint_path:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict()\n",
    "            }, checkpoint_path)\n",
    "    print(\"✅ Training complete.\")\n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "model, optimizer, scheduler = train_embedding_model(myModel, train_dl, num_epochs=50, class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709276fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = SoundDS(excluded_df)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51824ff2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EmbeddingClassifier' object has no attribute 'reset_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m val_acc\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Run inference on trained model with the validation set\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy)\n",
      "Cell \u001b[1;32mIn[65], line 6\u001b[0m, in \u001b[0;36minference\u001b[1;34m(model, val_dl)\u001b[0m\n\u001b[0;32m      4\u001b[0m val_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val_inputs, val_labels \u001b[38;5;129;01min\u001b[39;00m val_dl:\n\u001b[0;32m      7\u001b[0m         val_inputs \u001b[38;5;241m=\u001b[39m val_inputs\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m      8\u001b[0m         val_labels \u001b[38;5;241m=\u001b[39m val_labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[45], line 22\u001b[0m, in \u001b[0;36mSoundDS.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m class_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     21\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m AudioUtil\u001b[38;5;241m.\u001b[39mread_file(audio_file_path)\n\u001b[1;32m---> 22\u001b[0m aud,sr \u001b[38;5;241m=\u001b[39m \u001b[43mAudioUtil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_non_speech_segments\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aud \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m((idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))  \u001b[38;5;66;03m# Skip to next index safely\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 22\u001b[0m, in \u001b[0;36mAudioUtil.extract_non_speech_segments\u001b[1;34m(audio_file, segment_length_sec, no_of_segments)\u001b[0m\n\u001b[0;32m     19\u001b[0m waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Use Silero VAD to get timestamps of speech\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m speech_timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mget_speech_timestamps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Convert speech regions into a mask\u001b[39;00m\n\u001b[0;32m     25\u001b[0m speech_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(waveform, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\snakers4_silero-vad_master\\src\\silero_vad\\utils_vad.py:286\u001b[0m, in \u001b[0;36mget_speech_timestamps\u001b[1;34m(audio, model, threshold, sampling_rate, min_speech_duration_ms, max_speech_duration_s, min_silence_duration_ms, speech_pad_ms, return_seconds, time_resolution, visualize_probs, progress_tracking_callback, neg_threshold, window_size_samples)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently silero VAD models support 8000 and 16000 (or multiply of 16000) sample rates\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    284\u001b[0m window_size_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sampling_rate \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m16000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m--> 286\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_states\u001b[49m()\n\u001b[0;32m    287\u001b[0m min_speech_samples \u001b[38;5;241m=\u001b[39m sampling_rate \u001b[38;5;241m*\u001b[39m min_speech_duration_ms \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m    288\u001b[0m speech_pad_samples \u001b[38;5;241m=\u001b[39m sampling_rate \u001b[38;5;241m*\u001b[39m speech_pad_ms \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[1;32mc:\\KaggleComps\\BirdClef\\.a-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1942\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EmbeddingClassifier' object has no attribute 'reset_states'"
     ]
    }
   ],
   "source": [
    "def inference (model, val_dl):\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_dl:\n",
    "            val_inputs = val_inputs.to(device).float()\n",
    "            val_labels = val_labels.to(device).long()\n",
    "            val_inputs = (val_inputs - val_inputs.mean()) / val_inputs.std()\n",
    "\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_preds = val_outputs.argmax(dim=1)\n",
    "            val_correct += (val_preds == val_labels).sum().item()\n",
    "            val_total += val_labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        return val_acc\n",
    "\n",
    "# Run inference on trained model with the validation set\n",
    "accuracy = inference(trained_model, val_dl)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51281838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_tensor):\n",
    "    model.eval()\n",
    "    input_tensor = input_tensor.unsqueeze(0).to(device).float()\n",
    "    input_tensor = (input_tensor - input_tensor.mean()) / input_tensor.std()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_class = output.argmax(dim=1).item()\n",
    "        probs = torch.softmax(output, dim=1)  # [1, num_classes]\n",
    "    return predicted_class, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b222cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_audio/1192948/CSA36388.ogg\n",
      "128\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "t = 5\n",
    "\n",
    "filepaths = df['filename'].to_numpy()\n",
    "audio_file_path = filepaths[t]\n",
    "\n",
    "print(audio_file_path)\n",
    "\n",
    "res_classes = df['class'].to_numpy()\n",
    "req_class = res_classes[t]\n",
    "\n",
    "audio_file, sr = AudioUtil.read_file(audio_file_path)\n",
    "embeddings = AudioUtil.retrieve_embeddings(audio_file)\n",
    "\n",
    "label, probs = predict(trained_model, embeddings)\n",
    "probs = probs.numpy()\n",
    "print(req_class)\n",
    "print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".a-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
